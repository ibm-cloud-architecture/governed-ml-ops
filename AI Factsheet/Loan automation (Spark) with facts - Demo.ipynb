{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "#Import & initialize IBM AI Governance Python library & client "}, {"metadata": {}, "cell_type": "code", "source": "!pip install --extra-index-url https://test.pypi.org/simple/ ibm-aigov-facts-client", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import os\nPROJECT_UID= os.environ['PROJECT_ID']\nos.environ['FACTS_CLIENT_ENV'] = \"dev\";\nCONTAINER_ID=PROJECT_UID\nCONTAINER_TYPE=\"project\"\nEXPERIMENT_NAME=\"MyExperiment\"\nAPI_KEY = 'Your credentials go here'\nDB_PW = \"\"\"Your credentials go here\"\"\"\nDB_USER = 'Your credentials go here\nprint(PROJECT_UID)", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "bbafca27-1fc6-44aa-9c72-73c2044db2af\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from ibm_aigov_facts_client import AIGovFactsClient\nfacts_client = AIGovFactsClient(api_key=API_KEY,experiment_name=EXPERIMENT_NAME,container_type=CONTAINER_TYPE,container_id=CONTAINER_ID),set_as_current_experiment=True", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "2021/11/16 13:58:27 INFO : Experiment successfully created with ID 3 and name experiment5\n2021/11/16 13:58:27 INFO : Autolog enabled Successfully\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#Import  & initialize IBM WML library\nNon-IBM runtimes are supported by AI Governance as well"}, {"metadata": {}, "cell_type": "code", "source": "!pip install ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": API_KEY,\n    \"url\": 'https://us-south.ml.cloud.ibm.com'\n}", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\nwml_client = APIClient(wml_credentials)\nwml_client.set.default_project(PROJECT_UID)", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "'SUCCESS'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "data_asset_id = \"31ab850e-d6da-4852-9ab7-e4129b094643\" #Credit Risk Training data table\ndata_asset_path = \"https://dataplatform.dev.cloud.ibm.com/projects/\" + PROJECT_UID + \"/data-assets/\"+ data_asset_id + \"/preview?context=cpdaas\"\nprint(data_asset_path)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "https://dataplatform.dev.cloud.ibm.com/projects/bbafca27-1fc6-44aa-9c72-73c2044db2af/data-assets/31ab850e-d6da-4852-9ab7-e4129b094643/preview?context=cpdaas\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#Main section - regular ML development\nThe example below uses Spark-based learning of credit risk - but really anything can be done here.\nNo IBM code needs to be added to this main section - but AI Gov will still capture facts from there."}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark.sql import SparkSession\nexcept:\n    print('Error: Spark runtime is missing. If you are using Watson Studio change the notebook runtime to Spark.')\n    raise", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\nimport os\nfrom pyspark.sql import SparkSession\nsparkSession = (SparkSession.builder\n    .config(\"spark.jars.packages\", facts_client.ORG_FACTS_SPARK)\n    .getOrCreate())\n\nDB2_NWK38558_url = 'jdbc:db2://{}:{}/{}:sslConnection=true;'.format(\n    'b70af05b-76e4-4bca-a1f5-23dbb4c6a74e.c1ogj3sd0tgtu0lqde00.databases.appdomain.cloud',\n    32716,\n    'bludb'\n)\ndata_df = sparkSession.read.format('jdbc') \\\n    .option('url', DB2_NWK38558_url) \\\n    .option('dbtable', '\"NWK38558\".\"CREDIT_RISK_TRAINING\"') \\\n    .option('user', DB_USER) \\\n    .option('password', DB_PW).load()\ndata_df.show(5)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "+--------------+------------+--------------------+-----------+----------+---------------+------------------+------------------+------+------------+------------------------+-----------------+---+----------------+-------+--------------------+-------+----------+---------+-------------+-------+\n|CheckingStatus|LoanDuration|       CreditHistory|LoanPurpose|LoanAmount|ExistingSavings|EmploymentDuration|InstallmentPercent|   Sex|OthersOnLoan|CurrentResidenceDuration|     OwnsProperty|Age|InstallmentPlans|Housing|ExistingCreditsCount|    Job|Dependents|Telephone|ForeignWorker|   Risk|\n+--------------+------------+--------------------+-----------+----------+---------------+------------------+------------------+------+------------+------------------------+-----------------+---+----------------+-------+--------------------+-------+----------+---------+-------------+-------+\n|      0_to_200|          31|credits_paid_to_date|      other|      1889|     100_to_500|            less_1|                 3|female|        none|                       3|savings_insurance| 32|            none|    own|                   1|skilled|         1|     none|          yes|No Risk|\n|        less_0|          18|credits_paid_to_date|    car_new|       462|       less_100|            1_to_4|                 2|female|        none|                       2|savings_insurance| 37|          stores|    own|                   2|skilled|         1|     none|          yes|No Risk|\n|        less_0|          15|prior_payments_de...|  furniture|       250|       less_100|            1_to_4|                 2|  male|        none|                       3|      real_estate| 28|            none|    own|                   2|skilled|         1|      yes|           no|No Risk|\n|      0_to_200|          28|credits_paid_to_date| retraining|      3693|       less_100|         greater_7|                 3|  male|        none|                       2|savings_insurance| 32|            none|    own|                   1|skilled|         1|     none|          yes|No Risk|\n|   no_checking|          28|prior_payments_de...|  education|      6235|    500_to_1000|         greater_7|                 3|  male|        none|                       3|          unknown| 57|            none|    own|                   2|skilled|         1|     none|          yes|   Risk|\n+--------------+------------+--------------------+-----------+----------+---------------+------------------+------------------+------+------------+------------------------+-----------------+---+----------------+-------+--------------------+-------+----------+---------+-------------+-------+\nonly showing top 5 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark_df = data_df\n(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n\nMODEL_NAME = \"Loan automation (Spark)\"\nDEPLOYMENT_NAME = MODEL_NAME + \" - Deployment\"\n\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Number of records for training: 4005\nNumber of records for evaluation: 995\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\nfrom pyspark.ml.feature import SQLTransformer\n\nfeatures = [x for x in spark_df.columns if x != 'Risk']\ncategorical_features = ['CheckingStatus', 'CreditHistory', 'LoanPurpose', 'ExistingSavings', 'EmploymentDuration', 'Sex', 'OthersOnLoan', 'OwnsProperty', 'InstallmentPlans', 'Housing', 'Job', 'Telephone', 'ForeignWorker']\ncategorical_num_features = [x + '_IX' for x in categorical_features]\nsi_list = [StringIndexer(inputCol=x, outputCol=y) for x, y in zip(categorical_features, categorical_num_features)]\nva_features = VectorAssembler(inputCols=categorical_num_features + [x for x in features if x not in categorical_features], outputCol=\"features\")", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "si_label = StringIndexer(inputCol=\"Risk\", outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_label.labels)", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassifier\n\nclassifier = RandomForestClassifier(featuresCol=\"features\")\nfeature_filter = SQLTransformer(statement=\"SELECT * FROM __THIS__\")\npipeline = Pipeline(stages= si_list + [si_label, va_features, classifier, label_converter, feature_filter])\nmodel = pipeline.fit(train_data)", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "predictions = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(predictions)\n\nprint(\"areaUnderROC = %g\" % area_under_curve)", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "areaUnderROC = 0.730641\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# IBM WML code (defining model metadata)"}, {"metadata": {}, "cell_type": "code", "source": "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_2.4\")\nprint(\"Software Specification ID: {}\".format(software_spec_uid))\nmodel_props = {\n        wml_client._models.ConfigurationMetaNames.NAME:\"{}\".format(MODEL_NAME),\n        wml_client._models.ConfigurationMetaNames.TYPE: \"mllib_2.4\",\n        wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n        wml_client._models.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n        wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"Risk\",\n        wml_client._models.ConfigurationMetaNames.CUSTOM: { \"experiment_name\": EXPERIMENT_NAME}\n}", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Software Specification ID: 390d21f8-e58b-4fac-9c55-d7ceda621326\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#IBM AI Gov code to persist the auto-collected facts from main section"}, {"metadata": {}, "cell_type": "code", "source": "facts_client.export_facts.prepare_model_meta(wml_client=wml_client,meta_props=model_props)", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# IBM WML code:  persisting model as an asset"}, {"metadata": {}, "cell_type": "code", "source": "print(\"Storing model ...\")\npublished_model_details = wml_client.repository.store_model(\n    model=model, \n    meta_props=model_props, \n    training_data=train_data, \n    pipeline=pipeline)\n\nmodel_uid = wml_client.repository.get_model_uid(published_model_details)\nprint(\"Done\")\nprint(\"Model ID: {}\".format(model_uid))", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "Storing model ...\nDone\nModel ID: f50e3538-25c5-4672-9d04-0ce8fda133ed\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2020, 2021 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python37", "display_name": "Python 3.7 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}